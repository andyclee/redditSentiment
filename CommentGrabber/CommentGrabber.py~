""""
This script automatically acquires comments
from reddit and writes them to text files
in a directory so they can be processed
by Spark Streaming

NOTE: This method does *not* guarantee that
comments will not be duplicated
Given the amount of activity on reddit though,
duplication is unlikely
"""

import praw
import time
import json
import urllib.request

reddit = praw.Reddit('bot1')

allCommentsCurrent = "https://www.reddit.com/r/all/comments.json"

class commentQueue:
    
    def __init__(self):

        #Stores the list of comments to be written
        self.commentQ = []

    def pop(self):
        return self.commentQ.pop(0)

    def put(self, comments):
        return self.commentQ.append(comments)

acquiring = True

#Seconds in a day
initWaitTime = 86400

#Rate is limited at 1 request per 2 seconds
requestWait = 2

#Helper method for getIDFullList
def getCommentIDs():
    with urllib.request.urlopen(allCommentsCurrent) as url:
        commentID = []
        data = json.loads(url.read().decode())
        allComments = data["data"]["children"]

        for comment in allComments:
            kind = comment["kind"]
            commID = comment["data"]["id"]
            commLink = kind + "_" + commID
            commentID.append(commLink)

        return commentID

"""
Gets the full 100 ids to be processed later
"""
def getIDFullList():
    commentSet = []

    #An API info request can take up to 100 IDs at once
    #Each call from getCommentIDs returns 25 IDs
    for i in range(4):
        print(i)
        currentComments = getCommentIDs()
        commentSet.extend(currentComments)
        time.sleep(requestWait)

    return commentSet

for comment in reddit.subreddit('all').comments(limit=100):
    print(comment.author)
